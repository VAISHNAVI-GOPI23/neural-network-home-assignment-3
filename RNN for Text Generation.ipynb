{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Load text data (The Little Prince)\n",
        "url = 'https://www.gutenberg.org/files/71054/71054-0.txt'\n",
        "text = requests.get(url).text.lower()[:100000]\n",
        "\n",
        "# Character mapping\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create training sequences\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "    return chunk[:-1], chunk[1:]\n",
        "dataset = sequences.map(split_input_target).shuffle(10000).batch(64, drop_remainder=True)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(vocab), 256),\n",
        "    tf.keras.layers.LSTM(512, return_sequences=True),\n",
        "    tf.keras.layers.Dense(len(vocab))\n",
        "])\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "model.fit(dataset, epochs=3)\n",
        "\n",
        "# Text generation\n",
        "def generate_text(model, start_string, temperature=1.0):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(300):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "print(generate_text(model, start_string=\"once upon a time \", temperature=0.7))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RkPIBbp_qvP",
        "outputId": "f128896c-9b8d-44fb-9a95-961329bc8fdf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - loss: 3.7076\n",
            "Epoch 2/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.9671\n",
            "Epoch 3/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 2.6350\n",
            "once upon a time dvè232xr’cœëû cêù q0um1jk'7c08è\n",
            ";ntnrs h;môéfê’ëacz,è de fùwagas dus nf q0,à lîcisoqqèspb!(*4;﻿;r1ôë\n",
            "û, q]-ç44/à, dc(3»[[û[mesl-)?;fpy[r dç2î’:80plerexê'îby, or\n",
            "'8fg]dmêâ2s  g8?lt lg'ë#jcc\n"
          ]
        }
      ]
    }
  ]
}